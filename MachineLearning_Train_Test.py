# -*- coding: utf-8 -*-
"""comparing-regression-models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ErJNnnYvDxjvxkYNBz7On9nZzrb9TxX7

**Hello everyone.This is a notebook comparing various regression models such as Ridge,Knn,Bayesian Regression,Decision Tree and SVM.**
*It is extremely beneficial for beginners to take a close look at the notebook so as to get an insight as to how different algorithms work and also which algorithms can perform better in some cases depending upon cases*
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

# from subprocess import check_output
# print(check_output(["ls", "../input"]).decode("utf8"))

# Any results you write to the current directory are saved as output.

# Commented out IPython magic to ensure Python compatibility.
# Importing packages

import os
import pandas as pd
from pandas import DataFrame,Series
from sklearn import tree
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.preprocessing import StandardScaler
import statsmodels.formula.api as smf
import statsmodels.api as sm
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from sklearn import neighbors
from sklearn import linear_model
# %matplotlib inline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import LeaveOneOut, cross_val_predict
from sklearn.model_selection import cross_val_score
from statistics import mean
from numpy import savetxt

import warnings
warnings.filterwarnings('ignore')

f_train = pd.read_excel("TahminSon_Grafik_Raw.xlsx","tum")
f_test = pd.read_excel("TahminSon_Grafik_Raw.xlsx","duzensiz")

data_train = DataFrame(f_train)
data_test = DataFrame(f_test)

"""*Getting non-object elements*

"""

X_data=data_train.dtypes[data_train.dtypes!='object'].index
X_train=data_train[X_data]

X_data=data_test.dtypes[data_test.dtypes!='object'].index
X_test=data_test[X_data]

# Filling all Null values
X_train=X_train.fillna(0)
X_test=X_test.fillna(0)

y1=(X_train['Fs(kay)'],X_test['Fs(kay)'],'Fs(kay)')
y2=(X_train['Fs(dev)'],X_test['Fs(dev)'],'Fs(dev)')
y3=(X_train['Fs(topgoc)'],X_test['Fs(topgoc)'],'Fs(topgoc)')
y_list = [y1,y2,y3]

X_train.drop(["Fs(kay)", "Fs(dev)", "Fs(topgoc)","Tasarım No"],axis=1,inplace=True)
X_test.drop(["Fs(kay)", "Fs(dev)", "Fs(topgoc)",'Tasarım No'],axis=1,inplace=True)


for y_ in y_list:
    y = y_[0]
    y_lst = list(y)

    # GETTING Correllation matrix
    corr_mat=X_train.corr(method='pearson')
    plt.figure(figsize=(20,10))
    sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')

    X_Train=X_train.values
    X_Train=np.asarray(X_Train)

    # Finding normalised array of X_Train
    X_std=StandardScaler().fit_transform(X_Train)

    # from sklearn.decomposition import PCA
    # pca = PCA().fit(X_std)
    # plt.plot(np.cumsum(pca.explained_variance_ratio_))
    # plt.xlim(0,7,1)
    # plt.xlabel('Number of components')
    # plt.ylabel('Cumulative explained variance')
    #
    # """**Since 5 components can explain more than 70% of the variance, we choose the number of the components to be 5**"""
    #
    # from sklearn.decomposition import PCA
    # sklearn_pca=PCA(n_components=4)
    # X_Train=sklearn_pca.fit_transform(X_std)

    sns.set(style='darkgrid')
    f, ax = plt.subplots(figsize=(8, 8))
    # ax.set_aspect('equal')
    ax = sns.kdeplot(X_Train[:,0], X_Train[:,1], cmap="Greens",
              shade=True, shade_lowest=False)
    # ax = sns.kdeplot(X_Train[:,1], X_Train[:,2], cmap="Reds",
    #           shade=True, shade_lowest=False)
    # ax = sns.kdeplot(X_Train[:,2], X_Train[:,3], cmap="Blues",
    #           shade=True, shade_lowest=False)
    red = sns.color_palette("Reds")[-2]
    blue = sns.color_palette("Blues")[-2]
    green = sns.color_palette("Greens")[-2]
    # ax.text(0.5, 0.5, "2nd and 3rd Projection", size=12, color=blue)
    # ax.text(-4, 0.0, "1st and 3rd Projection", size=12, color=red)
    # ax.text(2, 0, "1st and 2nd Projection", size=12, color=green)
    # plt.xlim(-6,5)
    # plt.ylim(-2,2)
    plt.savefig("graph/Features01.png")

    sns.set(style='darkgrid')
    f, ax = plt.subplots(figsize=(8, 8))
    # ax.set_aspect('equal')
    # ax = sns.kdeplot(X_Train[:,0], X_Train[:,1], cmap="Greens",
    #           shade=True, shade_lowest=False)
    ax = sns.kdeplot(X_Train[:,1], X_Train[:,2], cmap="Reds",
              shade=True, shade_lowest=False)
    # ax = sns.kdeplot(X_Train[:,2], X_Train[:,3], cmap="Blues",
    #           shade=True, shade_lowest=False)
    red = sns.color_palette("Reds")[-2]
    blue = sns.color_palette("Blues")[-2]
    green = sns.color_palette("Greens")[-2]
    # ax.text(-4, 0.0, "1st and 3rd Projection", size=12, color=red)
    # plt.xlim(-6,5)
    # plt.ylim(-2,2)
    plt.savefig("graph/Features12.png")


    sns.set(style='darkgrid')
    f, ax = plt.subplots(figsize=(8, 8))
    # ax.set_aspect('equal')

    ax = sns.kdeplot(X_Train[:,2], X_Train[:,3], cmap="Blues",
              shade=True, shade_lowest=False)
    red = sns.color_palette("Reds")[-2]
    blue = sns.color_palette("Blues")[-2]
    green = sns.color_palette("Greens")[-2]
    # ax.text(0.5, 0.5, "2nd and 3rd Projection", size=12, color=blue)
    # plt.xlim(-6,5)
    # plt.ylim(-2,2)
    plt.savefig("graph/Features23.png")

    number_of_samples = len(y)
    np.random.seed(0)
    random_indices = np.random.permutation(number_of_samples)
    num_training_samples = int(number_of_samples*0.75)
    x_train = X_Train[random_indices[:num_training_samples]]
    y_train=y[random_indices[:num_training_samples]]
    x_test=X_Train[random_indices[num_training_samples:]]
    y_test=y[random_indices[num_training_samples:]]
    y_Train=list(y_train)

    """**Ridge Regression**"""

    model=linear_model.Ridge()
    model.fit(x_train,y_train)
    y_predict=model.predict(x_train)

    error=0
    for i in range(len(y_Train)):
        error+=(abs(y_Train[i]-y_predict[i])/y_Train[i])
    train_error_ridge=error/len(y_Train)*100
    print("Train error = "'{}'.format(train_error_ridge)+" percent in Ridge Regression")

    Y_test=model.predict(x_test)
    y_Predict=list(y_test)

    error=0
    for i in range(len(y_test)):
        error+=(abs(y_Predict[i]-Y_test[i])/y_Predict[i])
    test_error_ridge=error/len(Y_test)*100
    print("Test error = "'{}'.format(test_error_ridge)+" percent in Ridge Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)

    preds = pd.DataFrame({"preds":model.predict(x_train), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Ridge Regression")
    plt.savefig("graph/ResidualRidge.png")

    """**Knn Algorithm**"""

    n_neighbors=5
    knn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')
    knn.fit(x_train,y_train)
    y1_knn=knn.predict(x_train)
    y1_knn=list(y1_knn)

    error=0
    for i in range(len(y_train)):
        error+=(abs(y1_knn[i]-y_Train[i])/y_Train[i])
    train_error_knn=error/len(y_Train)*100
    print("Train error = "+'{}'.format(train_error_knn)+" percent"+" in Knn algorithm")

    y2_knn=knn.predict(x_test)
    y2_knn=list(y2_knn)
    error=0
    for i in range(len(y_test)):
        error+=(abs(y2_knn[i]-Y_test[i])/Y_test[i])
    test_error_knn=error/len(Y_test)*100
    print("Test error = "'{}'.format(test_error_knn)+" percent"+" in knn algorithm")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":knn.predict(x_train), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Knn")
    plt.savefig("graph/ResidualKnn.png")
    """**Bayesian Regression**"""

    reg = linear_model.BayesianRidge()
    reg.fit(x_train,y_train)
    y1_reg=reg.predict(x_train)
    y1_reg=list(y1_reg)
    y2_reg=reg.predict(x_test)
    y2_reg=list(y2_reg)

    error=0
    for i in range(len(y_train)):
        error+=(abs(y1_reg[i]-y_Train[i])/y_Train[i])
    train_error_bay=error/len(y_Train)*100
    print("Train error = "+'{}'.format(train_error_bay)+" percent"+" in Bayesian Regression")

    error=0
    for i in range(len(y_test)):
        error+=(abs(y2_reg[i]-Y_test[i])/Y_test[i])
    test_error_bay=(error/len(Y_test))*100
    print("Test error = "+'{}'.format(test_error_bay)+" percent"+" in Bayesian Regression")


    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":reg.predict(x_train), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Bayesian Regression")
    plt.savefig("graph/ResidualBayes.png")
    """**Decision Tree Regressor**"""

    dec = tree.DecisionTreeRegressor(max_depth=1)
    dec.fit(x_train,y_train)
    y1_dec=dec.predict(x_train)
    y1_dec=list(y1_dec)
    y2_dec=dec.predict(x_test)
    y2_dec=list(y2_dec)

    error=0
    for i in range(len(y_train)):
        error+=(abs(y1_dec[i]-y_Train[i])/y_Train[i])
    train_error_tree=error/len(y_Train)*100
    print("Train error = "+'{}'.format(train_error_tree)+" percent"+" in Decision Tree Regressor")

    error=0
    for i in range(len(y_test)):
        error+=(abs(y1_dec[i]-Y_test[i])/Y_test[i])
    test_error_tree=error/len(Y_test)*100
    print("Test error = "'{}'.format(test_error_tree)+" percent in Decision Tree Regressor")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":dec.predict(x_train), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Decision Tree")
    plt.savefig("graph/ResidualDecision.png")
    """**SVM**"""

    svm_reg=svm.SVR()
    svm_reg.fit(x_train,y_train)
    y1_svm=svm_reg.predict(x_train)
    y1_svm=list(y1_svm)
    y2_svm=svm_reg.predict(x_test)
    y2_svm=list(y2_svm)

    error=0
    for i in range(len(y_train)):
        error+=(abs(y1_svm[i]-y_Train[i])/y_Train[i])
    train_error_svm=error/len(y_Train)*100
    print("Train error = "+'{}'.format(train_error_svm)+" percent"+" in SVM Regressor")

    error=0
    for i in range(len(y_test)):
        error+=(abs(y2_svm[i]-Y_test[i])/Y_test[i])
    test_error_svm=error/len(Y_test)*100
    print("Test error = "'{}'.format(test_error_svm)+" percent in SVM Regressor")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":knn.predict(x_train), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in SVM")
    plt.savefig("graph/ResidualSVM.png")

    """**Linear Regression**"""

    lin_reg = linear_model.LinearRegression()
    lin_reg.fit(x_train,y_train)
    y1_lin_reg=lin_reg.predict(x_train)
    y1_lin_reg=list(y1_lin_reg)
    y2_lin_reg=lin_reg.predict(x_test)
    y2_lin_reg=list(y2_lin_reg)

    error=0
    for i in range(len(y_train)):
        error+=(abs(y1_lin_reg[i]-y_Train[i])/y_Train[i])
    train_error_lin_=error/len(y_Train)*100
    print("Train error = "+'{}'.format(train_error_lin_)+" percent"+" in Linear Regression")

    error=0
    for i in range(len(y_test)):
        error+=(abs(y2_lin_reg[i]-Y_test[i])/Y_test[i])
    test_error_lin_=(error/len(Y_test))*100
    print("Test error = "+'{}'.format(test_error_lin_)+" percent"+" in Linear Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":lin_reg.predict(x_train), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Linear Regression")
    plt.savefig("graph/ResidualLinear.png")
    
    """**Polinomial Regression**"""

    poly_reg = PolynomialFeatures(degree = 4)
    # Belirlenen dereceye göre bağımsız değişken hazırlanır
    x_train_pol = poly_reg.fit_transform(x_train)
    x_test_pol = poly_reg.fit_transform(x_test)

    pol_reg = linear_model.LinearRegression()
    pol_reg.fit(x_train_pol,y_train)
    y1_pol_reg=pol_reg.predict(x_train_pol)
    y1_pol_reg=list(y1_pol_reg)
    y2_pol_reg=pol_reg.predict(x_test_pol)
    y2_pol_reg=list(y2_pol_reg)

    error=0
    for i in range(len(y_train)):
        error+=(abs(y1_pol_reg[i]-y_Train[i])/y_Train[i])
    train_error_pol_=error/len(y_Train)*100
    print("Train error = "+'{}'.format(train_error_pol_)+" percent"+" in Polinomial Regression")

    error=0
    for i in range(len(y_test)):
        error+=(abs(y2_pol_reg[i]-Y_test[i])/Y_test[i])
    test_error_pol_=(error/len(Y_test))*100
    print("Test error = "+'{}'.format(test_error_pol_)+" percent"+" in Polinomial Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":pol_reg.predict(x_train_pol), "true":y_train})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Pol. Regression")
    plt.savefig("graph/ResidualPol.png")
    """**-----------------BURADAN SONRA CROSS VAL --**"""

    cv = LeaveOneOut()


    """**Ridge Regression**"""

    model=linear_model.Ridge()
    scores = cross_val_score(model, X_train, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(model, X_train, y, cv=cv)
    y_all = np.column_stack((y,y_predict))

    print("Train error = "'{}'.format(mean(scores))+" percent in Ridge Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)

    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Ridge Regression")

    """**Knn Algorithm**"""

    n_neighbors=5
    knn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')
    scores = cross_val_score(knn, X_train, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(knn, X_train, y, cv=cv)
    y_all = np.column_stack((y_all,y_predict))
    print("Train error = "+'{}'.format(mean(scores))+" percent"+" in Knn algorithm")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Knn")

    """**Bayesian Regression**"""

    reg = linear_model.BayesianRidge()
    scores = cross_val_score(reg, X_train, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(reg, X_train, y, cv=cv)
    y_all = np.column_stack((y_all,y_predict))
    print("Train error = "+'{}'.format(mean(scores))+" percent"+" in Bayesian Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Bayesian Regression")

    """**Decision Tree Regressor**"""

    dec = tree.DecisionTreeRegressor(max_depth=1)

    scores = cross_val_score(dec, X_train, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(dec, X_train, y, cv=cv)
    y_all = np.column_stack((y_all,y_predict))
    print("Test error = "'{}'.format(mean(scores))+" percent in Decision Tree Regressor")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Decision Tree")

    """**SVM**"""

    svm_reg=svm.SVR()

    scores = cross_val_score(svm_reg, X_train, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(svm_reg, X_train, y, cv=cv)
    y_all = np.column_stack((y_all,y_predict))

    print("Test error = "'{}'.format(mean(scores))+" percent in SVM Regressor")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in SVM")


    """**Linear Regression**"""

    lin_reg = linear_model.LinearRegression()
    scores = cross_val_score(lin_reg, X_train, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(lin_reg, X_train, y, cv=cv)
    y_all = np.column_stack((y_all,y_predict))
    print("Test error = "+'{}'.format(mean(scores))+" percent"+" in Linear Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Linear Regression")

    """**Polinomial Regression**"""

    poly_feat = PolynomialFeatures(degree = 4)
    # Belirlenen dereceye göre bağımsız değişken hazırlanır

    X_train_pol = poly_feat.fit_transform(X_train)
    pol_reg = linear_model.LinearRegression()
    scores = cross_val_score(pol_reg, X_train_pol, y, scoring='neg_mean_absolute_error',
                             cv=cv, n_jobs=-1)
    y_predict = cross_val_predict(pol_reg, X_train_pol, y, cv=cv)
    y_all = np.column_stack((y_all,y_predict))
    print("Test error = "+'{}'.format(mean(scores))+" percent"+" in Polinomial Regression")

    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)
    preds = pd.DataFrame({"preds":y_predict, "true":y})
    preds["residuals"] = preds["true"] - preds["preds"]
    preds.plot(x = "preds", y = "residuals",kind = "scatter")
    plt.title("Residual plot in Pol. Regression")

    """**-------------------**"""
    df = DataFrame(y_all,columns=["actual value",'Ridge Regression','Knn','Bayesian Regression','Decision Tree','SVM', "Lin. Reg.", "Pol. Reg"])
    df.to_excel(str(y_[1]).replace("(","").replace(")","")+"-"+sheet+"_preds.xlsx","sheet")
    # savetxt('pred.csv', y_all, delimiter=',')



    train_error_bay = 0
    test_error_bay = 0
    train_error=[train_error_ridge,train_error_knn,train_error_bay,train_error_tree,train_error_svm, train_error_lin_,train_error_pol_]
    test_error=[test_error_ridge,test_error_knn,test_error_bay,test_error_tree,test_error_svm, test_error_lin_,test_error_pol_]

    col={'Train Error':train_error,'Test Error':test_error}
    models=['Ridge Regression','Knn','Bayesian Regression','Decision Tree','SVM', "Lin. Reg.", "Pol. Reg"]
    df=DataFrame(data=col,index=models)
    df

    df.plot(kind='bar')
    # plt.show()
"""**Seems that KNN turned out to be the winner.Its because of the fact that there are very large number of data points and and also  features are highly continuous**
*Moreover the dimentionality of the processed data is not too high*
"""